首页

https://ipdbaike.com/

IPD基础体系

https://ipdbaike.com/?strategic/

IPD规划体系

https://ipdbaike.com/?planning/

IPD流程系

https://ipdbaike.com/?process/

IPD项目管理体系

https://ipdbaike.com/?project/

IPD绩效成长

https://ipdbaike.com/?performance/

IPD组织角色

https://ipdbaike.com/?role/

IPD咨询项目

https://ipdbaike.com/?zixun/

IPD培训业务

https://ipdbaike.com/?zxyw/

IPD专家推荐

https://ipdbaike.com/?zjtj/

IPD分享下载

https://ipdbaike.com/?download/



\cong\ipdbaike_crawler



__init__.py

```
"""
Crawler package for ipdbaike.com.
"""

```

config.py

```
from pathlib import Path

# Base settings
BASE_URL = "https://ipdbaike.com"
JINA_PREFIX = "https://r.jina.ai/"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Referer": BASE_URL + "/",
}

# Entry URLs (level 0)
START_URLS = [
    "https://ipdbaike.com/",
    "https://ipdbaike.com/?strategic/",
    "https://ipdbaike.com/?planning/",
    "https://ipdbaike.com/?process/",
    "https://ipdbaike.com/?project/",
    "https://ipdbaike.com/?performance/",
    "https://ipdbaike.com/?role/",
    "https://ipdbaike.com/?zixun/",
    "https://ipdbaike.com/?zxyw/",
    "https://ipdbaike.com/?zjtj/",
    "https://ipdbaike.com/?download/",
]

# Limits and paths
MAX_DEPTH = 3  # 0=home, 1=category, 2=list/pagination, 3=articles
MAX_PAGES = 300  # global safety cap
DELAY_SECONDS = 1.0

ROOT_DIR = Path(__file__).resolve().parent.parent
OUTPUT_ARTICLE_DIR = ROOT_DIR / "output" / "articles"
OUTPUT_ATTACHMENT_DIR = ROOT_DIR / "output" / "attachments"
LOG_DIR = ROOT_DIR / "logs"
LOG_FILE = LOG_DIR / "crawler.log"

# Attachment extensions to download
ATTACHMENT_EXTS = (".pdf", ".doc", ".docx", ".ppt", ".pptx", ".xls", ".xlsx", ".zip", ".rar")

# Regex patterns
ARTICLE_PATTERN = r"\.html$"
ATTACHMENT_PATTERN = r"https?://ipdbaike\.com/[^\s\"']+(?:%s)" % "|".join(ATTACHMENT_EXTS)

```



fetcher.py

```
import logging
from typing import Optional

import requests

from .config import HEADERS, JINA_PREFIX


def fetch_via_jina(url: str) -> Optional[str]:
    """Fetch page content via Jina Reader; returns markdown text or None."""
    jina_url = JINA_PREFIX + url
    try:
        resp = requests.get(jina_url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        logging.info("Jina fetch succeeded: %s", url)
        return resp.text
    except requests.RequestException as exc:
        logging.warning("Jina fetch failed: %s -> %s", url, exc)
        return None


def fetch_direct(url: str) -> Optional[str]:
    """Fetch page directly from origin; returns HTML text or None."""
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        resp.encoding = resp.apparent_encoding
        logging.info("Direct fetch succeeded: %s", url)
        return resp.text
    except requests.RequestException as exc:
        logging.error("Direct fetch failed: %s -> %s", url, exc)
        return None

```



main.py

```
import logging
import time
from pathlib import Path
from typing import Iterable, List, Optional

try:  # Allow both "python -m crawler.main" and "python crawler/main.py"
    from .config import (
        BASE_URL,
        DELAY_SECONDS,
        LOG_DIR,
        LOG_FILE,
        MAX_DEPTH,
        MAX_PAGES,
        START_URLS,
    )
    from .fetcher import fetch_direct, fetch_via_jina
    from .parser import (
        extract_attachment_links,
        extract_links_from_html,
        extract_links_from_markdown,
        is_article_page,
        is_list_page,
        normalize_url,
        pick_title_from_markdown,
    )
    from .queue_manager import CrawlQueue
    from .storage import save_attachment, save_markdown_article
except ImportError:  # fallback when run as a script without package context
    import sys
    from pathlib import Path

    ROOT = Path(__file__).resolve().parents[1]
    if str(ROOT) not in sys.path:
        sys.path.insert(0, str(ROOT))
    from crawler.config import (
        BASE_URL,
        DELAY_SECONDS,
        LOG_DIR,
        LOG_FILE,
        MAX_DEPTH,
        MAX_PAGES,
        START_URLS,
    )
    from crawler.fetcher import fetch_direct, fetch_via_jina
    from crawler.parser import (
        extract_attachment_links,
        extract_links_from_html,
        extract_links_from_markdown,
        is_article_page,
        is_list_page,
        normalize_url,
        pick_title_from_markdown,
    )
    from crawler.queue_manager import CrawlQueue
    from crawler.storage import save_attachment, save_markdown_article


def setup_logging() -> None:
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    handlers = [
        logging.FileHandler(LOG_FILE, encoding="utf-8"),
        logging.StreamHandler(),
    ]
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(message)s",
        handlers=handlers,
    )


def crawl(start_urls: Optional[Iterable[str]] = None, max_depth: int = MAX_DEPTH, max_pages: int = MAX_PAGES) -> None:
    queue = CrawlQueue()
    for url in start_urls or START_URLS:
        normalized = normalize_url(url)
        if normalized:
            queue.add(normalized, depth=0)

    while True:
        item = queue.pop()
        if not item:
            break
        url, depth = item
        if url in queue.visited:
            continue
        if depth > max_depth:
            continue
        if len(queue.visited) >= max_pages:
            logging.info("Reached max pages limit: %s", max_pages)
            break

        queue.mark_visited(url)
        logging.info("Crawl #%s depth=%s url=%s", len(queue.visited), depth, url)

        # Fetch via Jina first, fallback to direct HTML
        page_text = fetch_via_jina(url)
        content_is_md = page_text is not None
        if page_text is None:
            page_text = fetch_direct(url)
            content_is_md = False
        if page_text is None:
            continue

        # Download attachments if any
        for att_url in extract_attachment_links(page_text):
            save_attachment(att_url, referer=url)

        # Handle article or list
        if is_article_page(url):
            title = pick_title_from_markdown(page_text, url) if content_is_md else url
            save_markdown_article(url, title, page_text)
        elif is_list_page(url):
            if content_is_md:
                links = extract_links_from_markdown(page_text)
            else:
                links = extract_links_from_html(page_text, base=url)
            for link in links:
                queue.add(link, depth + 1)

        time.sleep(DELAY_SECONDS)

    logging.info("Crawling complete. Visited %s pages.", len(queue.visited))


def main() -> None:
    setup_logging()
    crawl()


if __name__ == "__main__":
    main()

```



parser.py

```
import re
from typing import Iterable, List
from urllib.parse import urljoin, urlparse

from bs4 import BeautifulSoup

from .config import ARTICLE_PATTERN, ATTACHMENT_PATTERN, ATTACHMENT_EXTS, BASE_URL


def normalize_url(url: str) -> str:
    """Normalize in-domain URL: strip fragments and trailing slash."""
    parsed = urlparse(url)
    if parsed.netloc and "ipdbaike.com" not in parsed.netloc:
        return ""
    clean = parsed._replace(fragment="")
    normalized = clean.geturl()
    if normalized.endswith("/") and not normalized.endswith("//"):
        normalized = normalized[:-1]
    return normalized


def looks_like_asset(url: str) -> bool:
    lower = url.lower()
    return any(lower.endswith(ext) for ext in ATTACHMENT_EXTS) or "/static/" in lower or "/images/" in lower


def extract_links_from_markdown(md: str) -> List[str]:
    """
    Pull links from markdown text (both markdown links and bare URLs).
    """
    candidates = set()
    # Markdown links: [text](url "title")
    candidates.update(re.findall(r"\((https?://ipdbaike\.com[^\s)]+)\)", md))
    # Bare URLs in text
    candidates.update(re.findall(r"https?://ipdbaike\.com[^\s)]+", md))

    links: List[str] = []
    for raw in candidates:
        normalized = normalize_url(raw.split("#")[0])
        if not normalized or looks_like_asset(normalized):
            continue
        links.append(normalized)
    return links


def extract_links_from_html(html: str, base: str = BASE_URL) -> List[str]:
    """Extract anchor links from HTML (absolute join)."""
    soup = BeautifulSoup(html, "html.parser")
    links: List[str] = []
    seen = set()
    for a in soup.find_all("a", href=True):
        raw = urljoin(base, a["href"])
        normalized = normalize_url(raw.split("#")[0])
        if not normalized or normalized in seen or looks_like_asset(normalized):
            continue
        seen.add(normalized)
        links.append(normalized)
    return links


def is_article_page(url: str) -> bool:
    return re.search(ARTICLE_PATTERN, url) is not None


def is_list_page(url: str) -> bool:
    return url.startswith(BASE_URL) and not is_article_page(url)


def extract_attachment_links(text: str) -> List[str]:
    return re.findall(ATTACHMENT_PATTERN, text)


def pick_title_from_markdown(md: str, default: str) -> str:
    """Get title from markdown 'Title:' header or first heading."""
    match = re.search(r"^Title:\s*(.+)$", md, re.M)
    if match:
        return match.group(1).strip()
    heading = re.search(r"^#\s+(.+)$", md, re.M)
    if heading:
        return heading.group(1).strip()
    return default

```



queue_manager.py

```
from collections import deque
from typing import Deque, Optional, Set, Tuple


class CrawlQueue:
    def __init__(self) -> None:
        self.queue: Deque[Tuple[str, int]] = deque()
        self.visited: Set[str] = set()

    def add(self, url: str, depth: int) -> None:
        if not url:
            return
        if url in self.visited:
            return
        self.queue.append((url, depth))

    def pop(self) -> Optional[Tuple[str, int]]:
        return self.queue.popleft() if self.queue else None

    def mark_visited(self, url: str) -> None:
        self.visited.add(url)

    def __len__(self) -> int:  # for convenience
        return len(self.queue)

```



storage.py

```
import logging
import os
from pathlib import Path
from typing import Optional

import requests

from .config import HEADERS, OUTPUT_ARTICLE_DIR, OUTPUT_ATTACHMENT_DIR


def _safe_title(title: str) -> str:
    return "".join(c if (c.isalnum() or c in (" ", "_", "-")) else "_" for c in title).strip()


def save_markdown_article(url: str, title: str, content: str) -> Path:
    OUTPUT_ARTICLE_DIR.mkdir(parents=True, exist_ok=True)
    safe_title = _safe_title(title) or "untitled"
    filepath = OUTPUT_ARTICLE_DIR / f"{safe_title}.md"
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(f"# {title}\n\nURL: {url}\n\n{content}")
    logging.info("Article saved: %s", filepath)
    return filepath


def save_attachment(file_url: str, referer: Optional[str] = None) -> Optional[Path]:
    OUTPUT_ATTACHMENT_DIR.mkdir(parents=True, exist_ok=True)
    local_name = file_url.split("/")[-1] or "attachment.bin"
    filepath = OUTPUT_ATTACHMENT_DIR / local_name
    if filepath.exists():
        logging.info("Attachment already exists, skip: %s", filepath)
        return filepath

    # Some endpoints enforce Referer anti-leech; send page URL when available.
    headers = dict(HEADERS)
    headers["Referer"] = referer or headers.get("Referer", "")

    try:
        with requests.get(file_url, headers=headers, stream=True, timeout=30) as resp:
            resp.raise_for_status()
            with open(filepath, "wb") as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
        logging.info("Attachment saved: %s", filepath)
        return filepath
    except requests.RequestException as exc:
        logging.error("Failed to download attachment %s -> %s", file_url, exc)
        if filepath.exists():
            try:
                filepath.unlink()
            except OSError:
                pass
        return None

```



微信搜索

```
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Standalone script to search WeChat public articles via Sogou, without importing project packages.

Usage:
    python wechat_search.py --query "新能源车" --limit 3 --pretty

Dependencies: requests, lxml
"""

import argparse
import json
import sys
import time
from typing import Dict, List, Any

import requests
from lxml import html
from urllib.parse import quote

VERIFY_SSL = True


HEADERS_COMMON = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36 Edg/137.0.0.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
    "Connection": "keep-alive",
}


def _get_with_fallback(url: str, *, params=None, headers=None, timeout: float = 15.0):
    """
    Do a GET request with optional HTTP fallback to mitigate SSL EOF errors from Sogou.
    """
    session = requests.Session()
    session.trust_env = False  # ignore system-level proxy settings
    try:
        return session.get(
            url,
            params=params,
            headers=headers,
            timeout=timeout,
            verify=VERIFY_SSL,
            proxies={"http": None, "https": None},
        )
    except requests.exceptions.SSLError:
        if url.startswith("https://"):
            http_url = "http://" + url[len("https://") :]
            return session.get(
                http_url,
                params=params,
                headers=headers,
                timeout=timeout,
                verify=False,
                proxies={"http": None, "https": None},
            )
        raise


def sogou_weixin_search(query: str) -> List[Dict[str, str]]:
    """
    Search WeChat articles on Sogou and return basic metadata.
    """
    headers = {
        **HEADERS_COMMON,
        "Referer": f"https://weixin.sogou.com/weixin?query={quote(query)}",
    }
    params = {
        "type": "2",
        "s_from": "input",
        "query": query,
        "ie": "utf8",
        "_sug_": "n",
        "_sug_type_": "",
    }
    response = _get_with_fallback("https://weixin.sogou.com/weixin", params=params, headers=headers, timeout=15)
    response.raise_for_status()

    tree = html.fromstring(response.text)
    results = []
    elements = tree.xpath("//a[contains(@id, 'sogou_vr_11002601_title_')]")
    publish_time = tree.xpath(
        "//li[contains(@id, 'sogou_vr_11002601_box_')]/div[@class='txt-box']/div[@class='s-p']/span[@class='s2']"
    )

    for element, time_elem in zip(elements, publish_time):
        title = element.text_content().strip()
        link = element.get("href")
        if link and not link.startswith("http"):
            link = "https://weixin.sogou.com" + link
        results.append(
            {
                "title": title,
                "link": link,
                "publish_time": time_elem.text_content().strip(),
            }
        )
    return results


def get_real_url(sogou_url: str) -> str:
    """
    Extract the real mp.weixin.qq.com article URL from the Sogou jump page.
    """
    headers = {
        **HEADERS_COMMON,
        "Cookie": "ABTEST=7; SUID=0A5BF4788E52A20B; IPLOC=CN1100; SUV=006817F578F45BFE",
    }
    resp = _get_with_fallback(sogou_url, headers=headers, timeout=15)
    resp.raise_for_status()

    script_content = resp.text
    start_index = script_content.find("url += '") + len("url += '")
    url_parts = []
    while True:
        part_start = script_content.find("url += '", start_index)
        if part_start == -1:
            break
        part_end = script_content.find("'", part_start + len("url += '"))
        part = script_content[part_start + len("url += '") : part_end]
        url_parts.append(part)
        start_index = part_end + 1

    full_url = "".join(url_parts).replace("@", "")
    if not full_url:
        return ""
    return "https://mp." + full_url


def get_article_content(real_url: str, referer: str) -> str:
    """
    Fetch article HTML and extract readable text.
    """
    headers = {
        **HEADERS_COMMON,
        "Referer": referer,
        "Upgrade-Insecure-Requests": "1",
    }
    resp = _get_with_fallback(real_url, headers=headers, timeout=15)
    resp.raise_for_status()
    tree = html.fromstring(resp.text)
    content_elements = tree.xpath("//div[@id='js_content']//text()")
    cleaned_content = [text.strip() for text in content_elements if text.strip()]
    return "\n".join(cleaned_content)


def get_wechat_article(query: str, number: int = 10) -> List[Dict[str, Any]]:
    """
    Search and fetch full articles (title, time, real_url, content).
    """
    start_time = time.time()
    results = sogou_weixin_search(query)
    if not results:
        return []
    articles = []
    for entry in results[:number]:
        sogou_link = entry["link"]
        real_url = get_real_url(sogou_link)
        content = get_article_content(real_url, referer=sogou_link) if real_url else ""
        articles.append(
            {
                "title": entry["title"],
                "publish_time": entry["publish_time"],
                "real_url": real_url,
                "content": content,
            }
        )
    duration = time.time() - start_time
    print(f"Fetched {len(articles)} articles for '{query}' in {duration:.2f}s")
    return articles


def main() -> None:
    parser = argparse.ArgumentParser(description="Search WeChat public articles (via Sogou).")
    parser.add_argument("-q", "--query", required=True, help="Keyword to search")
    parser.add_argument("-n", "--limit", type=int, default=5, help="Number of articles to fetch (default: 5)")
    parser.add_argument("--pretty", action="store_true", help="Pretty-print JSON output")
    parser.add_argument(
        "--insecure",
        action="store_true",
        help="Skip TLS verification and allow HTTP fallback (mitigate some SSL EOF errors)",
    )
    args = parser.parse_args()

    global VERIFY_SSL
    VERIFY_SSL = not args.insecure

    try:
        results = get_wechat_article(query=args.query, number=args.limit)
    except Exception as exc:  # noqa: BLE001
        print(f"Search failed: {exc}", file=sys.stderr)
        sys.exit(1)

    indent = 2 if args.pretty else None
    print(json.dumps(results, indent=indent, ensure_ascii=False))


if __name__ == "__main__":
    main()

```

